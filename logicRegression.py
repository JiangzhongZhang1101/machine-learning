import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import os
import time

path = 'E:/æ¢¯åº¦ä¸‹é™/'+'data' + os.sep + 'LogiReg_data.txt'
pdData = pd.read_csv(path,header=None,names=["Exam 1","Exam 2",'Admitted'])
#å› ä¸ºç¬¬ä¸€è¡Œä¸æ˜¯åˆ—åè€Œæ˜¯æ•°æ®ï¼Œæ‰€ä»¥æŠŠheaderè®¾ç½®ä¸ºNone
#æµ‹è¯•æ•°æ®æ˜¯å¦èƒ½å¤Ÿè¯»å–
#print(pdDate.head(10))
#print(pdDate.shape)
positive = pdData[pdData['Admitted'] == 1]# Admintted ä¸º1æ—¶è¡¨ç¤ºè¯¥æˆç»©è¢«å½•å–
negative = pdData[pdData['Admitted'] == 0]#Admintted ä¸º0æ—¶è¡¨ç¤ºè¯¥æˆç»©æœªè¢«å½•å–

#å±•çŽ°åˆå§‹æ•°æ®å›¾åƒ
# fig,ax = plt.subplots(figsize = (10,5))
# ax.scatter( positive['Exam 1'], positive['Exam 2'], s = 30, c = 'b', marker = 'o', label = 'Admintted')
# ax.scatter( negative['Exam 1'], negative['Exam 2'], s = 30, c = 'r', marker = "x", label = 'Not Admintted')
# ax.legend()#å·¦ä¸Šè§’æ ‡ç­¾
# ax.set_xlabel('Exam 1 Score')
# ax.set_ylabel('Exam 2 Score')
# ax.set_title('inital data')
# plt.show()

#è®¾ç½®sigmoid å‡½æ•° ð‘”(ð‘§)=1/(1+ð‘’^(âˆ’ð‘§))
#
def  sigmoid(z):
    '''
    sigmoid å‡½æ•° ð‘”(ð‘§)=1/(1+ð‘’^(âˆ’ð‘§))
    â„â†’[0,1] zâ†’(âˆ’âˆž,+âˆž)
    ð‘”(0)=0.5
    ð‘”(âˆ’âˆž)=0
    ð‘”(+âˆž)=1
    :param z:è¾“å…¥çš„æ•°å­—
    :return: zæ‰€å¯¹åº”çš„æ¦‚çŽ‡
    '''
    return 1 / (1 + np.exp(-z))
#æµ‹è¯•sigmoidå‡½æ•°
# nums = np.arange(-10,10,step = 0.1)
# fig,ax = plt.subplots(figsize=(10,5))
# ax.plot(nums,sigmoid(nums),'r')
# ax.set_title('sigmoid function')
# plt.show()

#è®¡ç®—Xä¸ŽÎ¸ç»„åˆåŽçš„æ¦‚çŽ‡
def model(X,theta):
    '''
    ä¼ é€’ä¸¤ä¸ªå‘é‡ï¼ˆX å’ŒÎ¸ï¼‰è¿›æ¥ï¼Œè¿›è¡Œç‚¹ä¹˜åŽä¼ å…¥sigmoidå‡½æ•°ä¸­ å³
    â„Žðœƒ(ð‘¥)=(ðœƒ0,ðœƒ1,ðœƒ2)âœ–(1,x1,x2)T = ðœƒ0+ ðœƒ1x1 + ðœƒ2x2
    :param X:è¾“å…¥æ•°æ®
    :param theta:æ•°æ®æƒé‡
    :return:åœ¨è¾“å…¥è¯¥æƒé‡ä¸‹æ•°æ®åŽçš„æ¦‚çŽ‡
    '''
    return sigmoid(np.dot(X,theta.T))

pdData.insert(0,'Ones',1)#æ·»åŠ ä¸€ä¸ªðœƒ0è¿™ä¸ªåˆ—æ‰€å¯¹åº”çš„Xå…¨ä¸º1  åœ¨ç¬¬0åˆ—å–åå«Ones æ•°æ®ä¸º1
orig_data = pdData.values #å°†æ•°æ®çš„pandaè¡¨ç¤ºå½¢å¼è½¬æ¢ä¸ºå¯¹æ•°ç»„å½¢å¼å³ä¸€è¡Œä¸ºä¸€ä¸ªæ•°ç»„ï¼Œ100ä¸ªæ•°ç»„ç»„æˆä¸€ä¸ªå¤§æ•°ç»„
#print(pdData)
#print(orig_data)
cols = orig_data.shape[1]#shape[0] æ˜¯èŽ·å–å½“å‰è¡Œæ•° shape[1] æ˜¯èŽ·å–å½“å‰åˆ—æ•°
X = orig_data[:,0:cols-1]#Xä¸ºç¬¬0åˆ—åˆ°ç¬¬cols-1åˆ— å³ç¬¬0åˆ—è‡³ç¬¬2åˆ— æ­¤å¤„Xï¼ˆ100ï¼Œ3ï¼‰
y = orig_data[:,cols-1:cols]#Yä¸ºç¬¬3åˆ—ï¼Œä½œä¸ºæ ‡ç­¾åˆ—ä½¿ç”¨  æ­¤å¤„ Y(100,1)
theta = np.zeros([1,3])#åˆ›å»ºä¸€ä¸ª1è¡Œ3åˆ—çš„ 0å‘é‡theta  æ­¤å¤„ theta (1,3)


#å¹³å‡æŸå¤±å‡½æ•°
def cost(X, y, theta):
    '''
    æŸå¤±å‡½æ•° æŸå¤±å‡½æ•°æ˜¯æ‰€æœ‰æ¨¡åž‹è¯¯å·®çš„å¹³æ–¹å’Œ
    D(hÎ¸(x),y)= âˆ’ylog(hÎ¸(x))âˆ’(1âˆ’y)log(1âˆ’hÎ¸(x))
    å¹³å‡æŸå¤±å‡½æ•° J(Î¸)
    J(Î¸)=1/ð‘›âˆ‘ð‘–=1 ð‘› ð·(â„Žðœƒ(ð‘¥ð‘–),ð‘¦ð‘–)
    :param X: è¾“å…¥æ•°æ®
    :param y: æ•°æ®æ ‡ç­¾
    :param theta: æ•°æ®æƒé‡
    :return: å¹³å‡æŸå¤±å‡½æ•° J(Î¸)
    '''
    left = np.multiply(-y, np.log(model(X, theta))) #âˆ’ð‘¦log(â„Žðœƒ(ð‘¥))
    right = np.multiply(1 - y, np.log(1 - model(X, theta))) #(1âˆ’y)log(1âˆ’hÎ¸(x))
    return np.sum(left-right)/(len(X)) #1/ð‘›âˆ‘ð‘–=1 ð‘› ð·(â„Žðœƒ(ð‘¥ð‘–),ð‘¦ð‘–)
#æµ‹è¯•costå‡½æ•°
#print(cost(X, y, theta))

#æ¢¯åº¦è®¡ç®—
#âˆ‚J/âˆ‚Î¸j =-1/mâˆ‘ð‘–=1 m (yiâˆ’hÎ¸(xi))xij
def gradient(X, y, theta): #è®¡ç®—æ¢¯åº¦ Xï¼ˆ100ï¼Œ3ï¼‰ Y(100,1) theta (1,3)
    '''
    :param X: è¾“å…¥æ•°æ®
    :param y: æ•°æ®æ ‡ç­¾
    :param theta: æ•°æ®æƒé‡
    :return: è¿”å›žå½“å‰ä½ç½®æ¢¯åº¦æœ€å¤§å€¼
    '''
    grad = np.zeros(theta.shape) #å¯¹åº”å’Œthetaç›¸åŒæ¢¯åº¦ï¼Œè¿™é‡Œæ˜¯3ä¸ªï¼ˆ1è¡Œ3åˆ—ï¼‰,å…ˆå…¨éƒ¨ç½®é›¶
    error = (model(X, theta) - y).ravel()# è®¡ç®—(â„Žðœƒ(ð‘¥ð‘–)-ð‘¦ð‘–) =-(ð‘¦ð‘–âˆ’â„Žðœƒ(ð‘¥ð‘–))ï¼Œç»“æžœè½¬æ¢ä¸ºä¸€ç»´æ•°ç»„ æ­¤å¤„æ˜¯è®¡ç®—
    for j in range(len(theta.ravel())): #å¾ªçŽ¯3æ¬¡
        term = np.multiply(error, X[:,j]) # è®©theta0ä¹˜ä»¥X0ï¼Œtheta1ä¹˜ä»¥X1ï¼Œtheta2ä¹˜ä»¥X2 åˆ†åˆ«è®¡ç®—é”™è¯¯çŽ‡
        grad[0,j] = np.sum(term)/len(X)  #æ±‚å’Œå–å¹³å‡
    return grad #è¿”å›žå½“å‰ä½ç½®æ¢¯åº¦æœ€å¤§å€¼

#æ¯”è¾ƒä¸‰ç§ä¸åŒæ¢¯åº¦ä¸‹é™æ–¹æ³•
STOP_ITER = 0 #æŒ‰ç…§è¿­ä»£æ¬¡æ•°è¿›è¡Œåœæ­¢
STOP_COST = 1 #æŒ‰ç…§æŸå¤±å‡½æ•°å‡ ä¹Žæ²¡å•¥å˜åŒ–åœæ­¢
STOP_GRAD = 2 #æŒ‰ç…§æ¢¯åº¦å‡ ä¹Žæ²¡å˜åŒ–åœæ­¢

def stopCriterion(type, value, threshold):
    """
    :param type: åœæ­¢æ–¹æ³•
    (STOP_ITERä¸ºæŒ‰ç…§è¿­ä»£æ¬¡æ•°è¿›è¡Œåœæ­¢;STOP_COST ä¸ºæŒ‰ç…§æŸå¤±å‡½æ•°å‡ ä¹Žæ²¡å•¥å˜åŒ–åœæ­¢ï¼›STOP_GRAD æŒ‰ç…§æ¢¯åº¦å‡ ä¹Žæ²¡å˜åŒ–åœæ­¢ï¼‰
    :param value:
    :param threshold:ç­–ç•¥å¯¹åº”é˜ˆå€¼
    :return:
    """
    if type == STOP_ITER:   return value > threshold
    elif type == STOP_COST: return abs(value[-1]-value[-2]) < threshold
    elif type == STOP_GRAD: return np.linalg.norm(value) < threshold

#æ•°æ®æ´—ç‰Œ,å¢žå¼ºæ³›åŒ–èƒ½åŠ›
def shuffleData(data):
    np.random.shuffle(data)
    cols = data.shape[1]
    X = data[:,:cols-1]
    y = data[:,cols-1:cols]
    return X,y

#æ¢¯åº¦ä¸‹é™æ±‚è§£
def descent(data, theta, batchSize, stopType, thresh, alpha):
    '''
    :param data: è¯»å–çš„æ•°æ®
    :param theta: æƒé‡
    :param batchSize: æ‰¹é‡å¤„ç†æ•°æ®çš„é•¿åº¦
    :param stopType: åœæ­¢ç­–ç•¥
    :param thresh: åœæ­¢ç­–ç•¥å¯¹åº”é˜ˆå€¼
    :param alpha:å­¦ä¹ çŽ‡
    :return:æ›´æ–°åŽçš„æƒé‡thetaï¼›å¾ªçŽ¯æ¬¡æ•°ï¼ŒæŸå¤±çŽ‡costsï¼Œæ¢¯åº¦å½“å‰gradï¼Œæ¶ˆè€—æ—¶é—´
    '''
    init_time = time.time()
    i = 0 #è¿­ä»£æ¬¡æ•°
    k = 0 #åˆå§‹æ‰¹é‡æ•°æ®é•¿åº¦
    X, y = shuffleData(data)
    grad = np.zeros(theta.shape)# è®¡ç®—çš„æ¢¯åº¦
    costs = [cost(X, y, theta)]# ç¬¬ä¸€æ¬¡çš„æŸå¤±å€¼
    n = len(y.ravel())
    while True:
        grad = gradient(X[k:k + batchSize], y[k:k + batchSize], theta)#å°æ‰¹é‡æ•°æ®æå–è¿›è¡Œæ¢¯åº¦è®¡ç®—
        k += batchSize#åˆå§‹æ‰¹é‡æ•°æ®é•¿åº¦æ¯æ¬¡å¢žåŠ batchSize
        if k >= n: #å½“ç´¯è®¡æ‰¹é‡æ•°æ®é•¿åº¦è¶…è¿‡æ•°æ®çš„é•¿åº¦åŽï¼Œè¿›è¡Œé‡æ–°æ´—ç‰Œ
            k = 0
            X,y = shuffleData(data) #é‡æ–°æ´—ç‰Œ
        #Î¸j = Î¸j - Î±1/mâˆ‘ð‘–=1 m (yiâˆ’hÎ¸(xi))xij
        theta = theta - alpha*grad #æ›´æ–°å‚æ•°
        costs.append(cost(X, y, theta))
        i += 1
        #åˆ¤æ–­æ˜¯å¦è¾¾åˆ°è·³å‡ºå¾ªçŽ¯æ¡ä»¶ï¼Œè¾¾åˆ°å°±è·³å‡º
        if stopType == STOP_ITER:   value = i
        elif stopType == STOP_COST: value = costs
        elif stopType == STOP_GRAD: value = grad
        if stopCriterion(stopType,value,thresh): break

    return theta, i-1, costs, grad, time.time()-init_time

#æ¢¯åº¦ä¸‹é™ç»“æžœå±•ç¤º
def runExpe(data, theta, batchSize, stopType, thresh, alpha):
    '''
    :param data: è¯»å–çš„æ•°æ®
    :param theta: æƒé‡
    :param batchSize: æ‰¹é‡å¤„ç†æ•°æ®çš„é•¿åº¦
    :param stopType: åœæ­¢ç­–ç•¥
    :param thresh: åœæ­¢ç­–ç•¥å¯¹åº”é˜ˆå€¼
    :param alpha:å­¦ä¹ çŽ‡
    :return: æ›´æ–°çš„æƒé‡
    '''
    theta, iter, costs, grad, dur = descent(data, theta, batchSize, stopType, thresh, alpha)
    name = "Original" if (data[:,1]>2).sum() > 1 else "Scaled"
    name += " data - learning rate: {} - ".format(alpha)
    if batchSize==len(y.ravel()): strDescType = "Gradient"
    elif batchSize==1:  strDescType = "Stochastic"
    else: strDescType = "Mini-batch ({})".format(batchSize)
    name += strDescType + " descent - Stop: "
    if stopType == STOP_ITER: strStop = "{} iterations".format(thresh)
    elif stopType == STOP_COST: strStop = "costs change < {}".format(thresh)
    else: strStop = "gradient norm < {}".format(thresh)
    name += strStop
    print ("***{}\nTheta: {} - Iter: {} - Last cost: {:03.2f} - Duration: {:03.2f}s".format(
        name, theta, iter, costs[-1], dur))
    fig, ax = plt.subplots(figsize=(12,4))
    ax.plot(np.arange(len(costs)), costs, 'r')
    ax.set_xlabel('Iterations')
    ax.set_ylabel('Cost')
    ax.set_title(name.upper() + ' - Error vs. Iteration')
    plt.show()
    return theta

#æ¢¯åº¦ä¸‹é™
n=100
#ä½¿ç”¨æŒ‰ç…§è¿­ä»£æ¬¡æ•°è¿›è¡Œåœæ­¢
#runExpe(orig_data, theta, n,STOP_ITER,thresh=10000,alpha=0.000001)
#Last cost: 0.63

#ä½¿ç”¨æŒ‰ç…§æŸå¤±å€¼åœæ­¢
#runExpe(orig_data, theta, n, STOP_COST, thresh=0.000001, alpha=0.001)
#Last cost: 0.38

#ä½¿ç”¨æ ¹æ®æ¢¯åº¦å˜åŒ–åœæ­¢
#runExpe(orig_data, theta, n, STOP_GRAD, thresh=0.05,alpha=0.001)
#Last cost: 0.49

#éšæœºæ¢¯åº¦ä¸‹é™
#runExpe(orig_data, theta, 1, STOP_ITER, thresh=5000, alpha=0.001)
#Last cost: 1.47 å¾ˆä¸ç¨³å®š,å†æ¥è¯•è¯•æŠŠå­¦ä¹ çŽ‡è°ƒå°ä¸€äº›

#runExpe(orig_data, theta, 1, STOP_ITER, thresh=15000, alpha=0.000001)
#Last cost: 0.63

#runExpe(orig_data, theta, 1, STOP_COST, thresh=0.0000001, alpha=0.0000001)
#Last cost: 0.63

#å°æ‰¹é‡æ¢¯åº¦ä¸‹é™
#runExpe(orig_data, theta, 16, STOP_ITER, thresh=15000, alpha=0.001)
#Last cost: 0.66 æµ®åŠ¨ä»ç„¶è¾ƒå¤§

#å¯¹æ•°æ®è¿›è¡Œæ ‡å‡†åŒ– å°†æ•°æ®æŒ‰å…¶å±žæ€§(æŒ‰åˆ—è¿›è¡Œ)å‡åŽ»å…¶å‡å€¼ï¼Œç„¶åŽé™¤ä»¥å…¶æ–¹å·®ã€‚
# æœ€åŽå¾—åˆ°çš„ç»“æžœæ˜¯ï¼Œå¯¹æ¯ä¸ªå±žæ€§/æ¯åˆ—æ¥è¯´æ‰€æœ‰æ•°æ®éƒ½èšé›†åœ¨0é™„è¿‘ï¼Œæ–¹å·®å€¼ä¸º1
from sklearn import preprocessing as pp
scaled_data = orig_data.copy()
scaled_data[:, 1:3] = pp.scale(orig_data[:, 1:3])
#runExpe(scaled_data, theta, n, STOP_ITER, thresh=5000, alpha=0.001)
#Last cost: 0.38
#runExpe(scaled_data, theta, 16, STOP_GRAD, thresh=0.002, alpha=0.0005)
#Last cost: 0.20   Duration: 169.45s
#éšæœºæ¢¯åº¦ä¸‹é™æ›´å¿«ï¼Œä½†æ˜¯æˆ‘ä»¬éœ€è¦è¿­ä»£çš„æ¬¡æ•°ä¹Ÿéœ€è¦æ›´å¤šï¼Œæ‰€ä»¥è¿˜æ˜¯ç”¨batchçš„æ¯”è¾ƒåˆé€‚ï¼ï¼
runExpe(scaled_data, theta, 16, STOP_GRAD, thresh=0.002*2, alpha=0.001)
#Last cost: 0.21

#ç²¾åº¦
#è®¾å®šé˜ˆå€¼
def predict(X, theta):
    return [1 if x >= 0.5 else 0 for x in model(X, theta)]
scaled_X = scaled_data[:, :3]
y = scaled_data[:, 3]
predictions = predict(scaled_X, theta)
correct = [1 if ((a == 1 and b == 1) or (a == 0 and b == 0)) else 0 for (a, b) in zip(predictions, y)]
accuracy = (sum(map(int, correct)) % len(correct))
print ('accuracy = {0}%'.format(accuracy))